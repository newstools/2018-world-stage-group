By Niyi Tayo– Machine learning and artificial intelligence will be key enablers in a new era of digital experiences and value propositions, but the revolution needs a human touch. Keeping interactions between your artificial intelligence (AI) and your clients fair and unbiased will be critical to the success of your business. Even though machine learning and AI will be key requirements in your future business, the digital revolution needs a human touch. Why are human values so important to the success of AI, and what does this mean for your business? What can you do to eliminate bias in machine learning and ensure that your AI behaves as a good citizen? We will dive into Citizen AI, as illustrated in Accenture’s Technology Vision for 2018. The AI revolution needs a human touch AI is expanding into every facet of our lives, driving better business decisions and user experiences throughout a wide range of markets and industries. AI has effectively become a new user interface, making our interactions with the increasingly advanced technologies that surround us easier and more seamless. Intelligent digital assistants like Google Now in our smartphones, helping us reach out to others, find information, make notes and navigate the physical world. Meanwhile, Amazon Alexa and Google Home have given us the ability to order goods and services without ever seeing a screen. These developments are constantly making it easier to use advanced technology effectively. Instead of adapting to our machines, we are teaching our machines to adapt to us. But as we train our artificial intelligences to approve loans, identify job candidates and treat patients, we must be mindful of our responsibility to society. In a future where AI has the power to facilitate every touch point from commerce to public services, we must strive to ensure it remains a force for good. We, therefore, have to nurture our AI as with great power comes great responsibility. We call it “Citizen AI”. The Clear and Present Danger of Bias In the simplest terms, an AI is a learning system. It grows and matures through data, acquiring new insights and capabilities based on the inputs we feed into it. In order to guide the training process in the right direction, data scientists carefully tailor those datasets to promote the goals an AI is meant to achieve. But that data can also confer unconscious bias and hidden prejudice. Last year, researchers at Carnegie Mellon found that Google’s advertising algorithms were six times more likely to display ads for high-income jobs to men than they were to women. In another well-publicized incident, the first version of Google Photos tagged people of color as gorillas. And when ProPublica dug into the workings of COMPAS, a tool used to predict recidivism rates among criminals as part of the parole process, they were shocked to discover it discriminated against minorities. These are extreme examples, to be sure, but they illuminate a fundamental challenge in AI. As we continue to expand our use of these technologies, we must create a strong ethical framework for AI and maintain control over the choices it makes. Unlocking the Black Box Training AI is a complicated process. Take deep learning, for instance: a pattern recognition method that mimics the way the human brain works and applies those principles to neural networking. This technique has allowed businesses to make substantial progress in highly complex fields such as image recognition. Where humans see a cat, a goat or a tomato, a computer will only see a field of pixels. Deep learning has given computers the ability to recognize what those pixels represent – but we can’t fully explain how the system works, even though we programmed the basics ourselves. This is a surprisingly common phenomenon. The neural network powering Google Translate invented its own common language to aid in translations, even though it wasn’t specifically instructed to do so. Naturally, this lack of insight into the inner workings of AI is rarely a problem when the results are positive. If efficiency is up and users are happy, why worry? But the case of Microsoft’s Tay chatbot reveals that things aren’t quite so simple. Within just 24 hours of the chatbot’s Twitter debut, it had transformed from a capable conversationalist into an equally capable racist. Although the project was intended as an experiment from the start, it clearly illustrates the potential for disaster when AI is sent out into the world without proper guidance. These AI examples can apply to biases in Nigeria as well.  Biases in terms of ethnicity, religion, place of residence, income levels etc.  Ensuring that AI solutions deployed by organisations are free from these biases is important as the impact of not doing so goes beyond financial to the brand of the companies. It goes without saying that parents are highly motivated to raise their children well and teach them how to be responsible adults. Companies should feel the same duty of care towards their AIs. Raising Responsible AI In order to seize the growth opportunities that AI provides, your company will need to address core ethical considerations and establish a set of value-driven requirements to guide the deployment of your AI. At Accenture, we’ve developed the Responsible AI approach to help our clients create the proper governance frameworks to evaluate, deploy and monitor AI. Our methodology focuses on architecture and solutions that emphasize people and human values. Using this approach will help you guide the implementation of AI in a positive direction and establish the prerequisites for future growth. AI must be anchored to your core values as a company. Establishing strong governance with clear ethical guardrails and accountability frameworks will allow your artificial intelligence to flourish. Consumers will only be able to trust your AI as far as you can explain its actions and decisions. Achieving that trust requires privacy, transparency and security to be built into the design by definition. Once your AI is out in the wild, monitoring its performance against value-driven metrics is critical to its success – and yours. You must guard against bias while ensuring accountability and security. Introducing AI will impact individuals within your organization as well. With Accenture’s myLearning technology, you can democratize AI learning and reduce barriers to entry. Why Responsibility is Key In the coming years, organizations such as your own will have an enormous opportunity to weave technology into the fabric of our lives in ways that are responsible, empowering and fair. But your ability to reap the rewards of AI will depend on your ability to guide and control it. This reaches beyond the realm of laws and regulations. If your business implements AI whose decisions will have an impact on the lives of your customers, you must be mindful of the values you instill in those autonomous systems. Consumers will expect you to be able to explain how and why your AI makes decisions. You will need to be aware of its values. You will need to establish strong governance, monitor performance and test for hidden bias and undesirable traits. And you will need to be able to correct any issues you discover quickly. This will not be an easy task. Understanding AI will require new skills and insights. But if you succeed, your clients will be even more willing to accept these technologies into their lives. This, in turn, will allow you to get much closer to them and provide better services than your competitors, giving you an invaluable advantage in today’s hyper-personalized markets. *Niyi Tayo is the  Managing Director, Technology Accenture Nigeria